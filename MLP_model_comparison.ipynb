{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries for the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, cohen_kappa_score, f1_score, confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = pd.read_csv('9070_training_af.csv')  # Training set\n",
    "set2 = pd.read_csv('3024_internal validation_af.csv')  # Internal validation set\n",
    "set3 = pd.read_csv('3024_external_validation_af.csv') # External validation set\n",
    "data = pd.read_csv(\"dataset_merge_for_cv.csv\")  # Merged Training and Internal validation set for cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = set1.drop(columns=['Smiles', 'Activity'])\n",
    "Y_train = set1['Activity'].map({'Active': 1, 'Inactive': 0})\n",
    "\n",
    "y_val = set2.drop(columns=['Smiles', 'Activity'])\n",
    "y_val_labels = set2['Activity'].map({'Active': 1, 'Inactive': 0})\n",
    "\n",
    "z_test = set3.drop(columns=['Smiles', 'Activity'])\n",
    "z_test_labels = set3['Activity'].map({'Active': 1, 'Inactive': 0})\n",
    "\n",
    "X = data.drop(columns=['Smiles', 'Activity'])\n",
    "Y = data['Activity'].map({'Active': 1, 'Inactive': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)\n",
    "print(\"Shape of y_val_labels:\", y_val_labels.shape)\n",
    "print(\"Shape of z_test:\", z_test.shape)\n",
    "print(\"Shape of z_test_labels:\", z_test_labels.shape)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [(150, 100, 50), (50, 50), (100, 100), (100,)]\n",
    "activations = ['relu', 'tanh']\n",
    "alphas = [0.0001, 0.001, 0.01]\n",
    "learning_rates = ['constant','adaptive']\n",
    "\n",
    "# Creating all combinations of parameters\n",
    "param_combinations = list(itertools.product(hidden_layer_sizes, activations, alphas, learning_rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing variables to track the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "accuracy_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, params in enumerate(param_combinations):\n",
    "    print(f\"Training model {idx+1}/{len(param_combinations)} with params: {params}\", flush=True)\n",
    "    hls, activation, alpha, lr = params\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hls,\n",
    "        activation=activation,\n",
    "        alpha=alpha,\n",
    "        learning_rate=lr,\n",
    "        random_state=42,\n",
    "        verbose=True  \n",
    "        )\n",
    "        \n",
    "    model.fit(X_train, Y_train)\n",
    "        \n",
    "    y_val_predictions = model.predict(y_val)\n",
    "    accuracy = accuracy_score(y_val_labels, y_val_predictions)\n",
    "        \n",
    "    accuracy_data.append({\n",
    "        'hidden_layer_sizes': hls,\n",
    "        'activation': activation,\n",
    "        'alpha': alpha,\n",
    "        'learning_rate': lr,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "    # Tracking the best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_model = model\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "    \n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Internal Validation Accuracy:\", best_accuracy)\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_data)\n",
    "accuracy_df.to_csv('model_accuracies.csv', index=False)\n",
    "print(\"\\nModel accuracy data has been saved to 'model_accuracies.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the best model on external validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test_predictions = best_model.predict(z_test)\n",
    "z_test_probabilities = best_model.predict_proba(z_test)[:, 1]  # Probabilities for ROC AUC score\n",
    "\n",
    "accuracy = accuracy_score(z_test_labels, z_test_predictions)\n",
    "precision = precision_score(z_test_labels, z_test_predictions)\n",
    "recall = recall_score(z_test_labels, z_test_predictions)\n",
    "f1 = f1_score(z_test_labels, z_test_predictions)\n",
    "roc_auc = roc_auc_score(z_test_labels, z_test_probabilities)\n",
    "\n",
    "evaluation_metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'roc_auc': roc_auc\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Model Evaluation on External Validation Set (z_test):\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "evaluation_df.to_csv('best_model_evaluation.csv', index=False)\n",
    "print(\"\\nBest model's evaluation metrics have been saved to 'best_model_evaluation.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ROC AUC Curve for the final MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(z_test_labels, z_test_probabilities)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('ROC AUC Curve for Best Model on External Validation Set')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.savefig('roc_auc_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "10-Fold Cross Validation for the final MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls, activation, alpha, lr = best_params\n",
    "\n",
    "all_seed_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for running the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(10):\n",
    "    print(f\"\\nStarting 10-fold CV with random seed {seed}\")\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    \n",
    "    seed_metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'sensitivity': [],\n",
    "        'specificity': [],\n",
    "        'cohen_kappa': [],\n",
    "        'f1_score': [],\n",
    "        'auroc': []\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        \n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=hls,\n",
    "            activation=activation,\n",
    "            alpha=alpha,\n",
    "            learning_rate=lr,\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        sensitivity = recall_score(y_test, y_pred) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auroc = roc_auc_score(y_test, y_prob)\n",
    "        cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "        \n",
    "        # Calculating specificity\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        seed_metrics['accuracy'].append(accuracy)\n",
    "        seed_metrics['precision'].append(precision)\n",
    "        seed_metrics['sensitivity'].append(sensitivity)\n",
    "        seed_metrics['specificity'].append(specificity)\n",
    "        seed_metrics['cohen_kappa'].append(cohen_kappa)\n",
    "        seed_metrics['f1_score'].append(f1)\n",
    "        seed_metrics['auroc'].append(auroc)\n",
    "        \n",
    "        print(f\"Seed {seed}, Fold {fold + 1}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, Cohen’s Kappa: {cohen_kappa:.4f}, F1-Score: {f1:.4f}, AUROC: {auroc:.4f}\")\n",
    "    \n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    seed_results = {\n",
    "        'seed': seed,\n",
    "        'accuracy_mean': np.mean(seed_metrics['accuracy']),\n",
    "        'accuracy_std': np.std(seed_metrics['accuracy']),\n",
    "        'precision_mean': np.mean(seed_metrics['precision']),\n",
    "        'precision_std': np.std(seed_metrics['precision']),\n",
    "        'sensitivity_mean': np.mean(seed_metrics['sensitivity']),\n",
    "        'sensitivity_std': np.std(seed_metrics['sensitivity']),\n",
    "        'specificity_mean': np.mean(seed_metrics['specificity']),\n",
    "        'specificity_std': np.std(seed_metrics['specificity']),\n",
    "        'cohen_kappa_mean': np.mean(seed_metrics['cohen_kappa']),\n",
    "        'cohen_kappa_std': np.std(seed_metrics['cohen_kappa']),\n",
    "        'f1_score_mean': np.mean(seed_metrics['f1_score']),\n",
    "        'f1_score_std': np.std(seed_metrics['f1_score']),\n",
    "        'auroc_mean': np.mean(seed_metrics['auroc']),\n",
    "        'auroc_std': np.std(seed_metrics['auroc'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nSeed {seed} Results:\")\n",
    "    for metric, value in seed_results.items():\n",
    "        if metric != 'seed':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    all_seed_results.append(seed_results)\n",
    "\n",
    "results_df = pd.DataFrame(all_seed_results)\n",
    "results_df.to_csv(\"cv_results_with_metrics.csv\", index=False)\n",
    "print(\"\\nCross-validation results with metrics have been saved to 'cv_results_with_metrics.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
